[[_architecture]]
= Architecture

{project-title} is essentially an {link_etl} tool where data is extracted from the source system, transformed (see <<_processing,Processing>>), and loaded into the target system.

image::architecture.svg[]

[[_batching]]
== Batching

Processing in {project-title} is done in batches: a fixed number of records is read from the source, processed, and written to the target.
The default batch size is `50`, which means that an execution step reads 50 items at a time from the source, processes them, and finally writes then to the target.
If the target is Redis, writing is done in a single command ({link_redis_pipelining}) to minimize the number of roundtrips to the server.

You can change the batch size (and hence pipeline size) using the `--batch` option.
The optimal batch size in terms of throughput depends on many factors like record size and command types (see {link_pipeline_tuning} for details).

[[_threads]]
== Multi-threading

It is possible to parallelize processing by using multiple threads.
In that configuration, each chunk of items is read, processed, and written in a separate thread of execution.
This is different from partitioning where items would be read by multiple readers.
Here, only one reader is being accessed from multiple threads.

To set the number of threads, use the `--threads` option.

.Multi-threading example
[source]
----
include::{testdir}/db-import-postgresql-multithreaded[]
----

[[_processing]]
== Processing

{project-title} lets you transform incoming records using processors.
These processors allow you to create/update/delete fields using the {link_spel} (SpEL).
For example, import commands like `file-import`, `database-import`, and `faker-import` have a `--proc` option that allow for field-level processing:

* `field1='foo'` -> generate a field named `field1` containing the string `foo`
* `temp=(temp-32)*5/9` -> convert from Fahrenheit to Celsius
* `name=remove(first).concat(remove(last))` -> concatenate `first` and `last` fields and delete them
* `field2=null` -> delete `field2`

Input fields are accessed by name (e.g. `field3=field1+field2`).

Processors have access to the following context variables and functions:

`date`:: Date parsing and formatting object.
Instance of Java https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html[SimpleDateFormat].

`redis`:: Redis commands object.
Instance of Lettuce https://www.lettuce.io/core/release/api/io/lettuce/core/api/sync/RedisCommands.html[RedisCommands].

`geo`:: Convenience function that takes a longitude and a latitude to produce a RediSearch geo-location string in the form `longitude,latitude` (e.g. `location=#geo(lon,lat)`)

.Processor example
[source]
----
riot file-import --proc epoch="#date.parse(mydate).getTime()" location="#geo(lon,lat)" name="#redis.hget('person1','lastName')" ...
----

You can register your own variables using `--var`.

.Custom variable example
[source]
----
include::{testdir}/file-import-process-var[]
----

[[_filters]]
== Filtering

Filters allow you to exclude records that don't match a SpEL boolean expression.

For example this filter will only keep records where the `value` field is a series of digits:

[source]
----
riot file-import --filter "value matches '\\d+'" ...
----

[[_replication_architecture]]
== Replication

image::replication-architecture.svg[]

The basic replication mechanism is as follows:

1. Identify source keys to be replicated using scan and/or keyspace notifications depending on the <<_replication_mode,replication mode>>.

2. Read data associated with each key using <<_replication_dump_restore,dump>> or <<_replication_type_based,type-specific commands>>.

3. Write each key to the target using <<_replication_dump_restore,restore>> or <<_replication_type_based,type-specific commands>>.